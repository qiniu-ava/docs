- [AVA 整体实施方案](#ava-%E6%95%B4%E4%BD%93%E5%AE%9E%E6%96%BD%E6%96%B9%E6%A1%88)
  - [前言](#%E5%89%8D%E8%A8%80)
  - [系统架构](#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84)
  - [系统规划](#%E7%B3%BB%E7%BB%9F%E8%A7%84%E5%88%92)
    - [存储系统](#%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F)
    - [计算节点](#%E8%AE%A1%E7%AE%97%E8%8A%82%E7%82%B9)
    - [系统环境](#%E7%B3%BB%E7%BB%9F%E7%8E%AF%E5%A2%83)
    - [其他依赖](#%E5%85%B6%E4%BB%96%E4%BE%9D%E8%B5%96)
  - [安装部署](#%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2)
    - [(一) 机房规划](#%E4%B8%80-%E6%9C%BA%E6%88%BF%E8%A7%84%E5%88%92)
    - [(二) 部署 Prometheus 监控服务](#%E4%BA%8C-%E9%83%A8%E7%BD%B2-prometheus-%E7%9B%91%E6%8E%A7%E6%9C%8D%E5%8A%A1)
    - [(三) 搭建存储集群](#%E4%B8%89-%E6%90%AD%E5%BB%BA%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4)
    - [(四) 搭建缓存系统](#%E5%9B%9B-%E6%90%AD%E5%BB%BA%E7%BC%93%E5%AD%98%E7%B3%BB%E7%BB%9F)
    - [(五) 部署 kubernetes](#%E4%BA%94-%E9%83%A8%E7%BD%B2-kubernetes)
    - [(六) 部署 AVA 服务](#%E5%85%AD-%E9%83%A8%E7%BD%B2-ava-%E6%9C%8D%E5%8A%A1)
  - [上线运行](#%E4%B8%8A%E7%BA%BF%E8%BF%90%E8%A1%8C)
    - [(一) AVA 管理员](#%E4%B8%80-ava-%E7%AE%A1%E7%90%86%E5%91%98)
    - [(二) AVA 用户](#%E4%BA%8C-ava-%E7%94%A8%E6%88%B7)
  - [系统验收](#%E7%B3%BB%E7%BB%9F%E9%AA%8C%E6%94%B6)
    - [存储系统](#%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9F-1)
      - [cephfs](#cephfs)
      - [ceph-rbd](#ceph-rbd)
    - [调度系统](#%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F)
    - [其他特性](#%E5%85%B6%E4%BB%96%E7%89%B9%E6%80%A7)





















# AVA 整体实施方案

## 前言

七牛云 AVA 深度学习平台是面向深度学习训练任务的用户、计算、存储资源管理平台，不仅能大幅提升计算与存储资源的利用率，还能从业务层面对平台用户的各种资源进行统计与限额管理。

AVA 系统采用计算存储分离的模式来保证平台的扩展性，本文将以 ceph 存储系统和七牛云对象存储服务 Kodo 私有化版本为例介绍存储系统的搭建过程，以 kubernetes 1.9 版本为例介绍计算集群的搭建过程。

## 系统架构

![ava 架构图](http://p0ryklpfl.bkt.clouddn.com/ava.png)

## 系统规划

为保证整个系统的高效稳定运行，我们需要计算、存储、网络和系统等多个方面的保障。从贵司的资源规划情况出发，我们认为要释放出 DGX-1 的强大计算性能，可以使用如下资源配置。

### 存储系统

使用两副本模式部署 ceph 13.2.x 版本，推荐的具体硬件指标如下:

* 纯储存节点，用于部署 ceph monitor 与 ceph OSD，推荐全部使用 SSD 资源，视实际容量需求决定节点数量
  * CPU 36 cores
  * memory 128Gi 起(内存与 SSD 数量成正比，每块 SSD 对应 8~10Gi)
  * 240Gi SSD + 4Ti SSD * 12 起
  * 至少1张万兆网卡
* Cephfs metadata server 节点，3台或者5台
  * CPU 48 cores
  * memory 256Gi 起 (内存需求高)
  * 240Gi SSD + 800Gi SSD
  * 万兆网卡或者千兆网卡皆可
* 私有化对象存储服务，可以使用七牛云 Kodo 私有部署的方案引入对象存储，具体实施方案可参考【对象存储私有化部署】
* 在使用对象存储时，推荐使用 Alluxio 缓存系统来加速数据读取。Alluxio 是一个可以将各种底层存储系统(如 七牛云存储kodo、ceph、S3A)的数据缓存到 Mem-SSD-HDD 的多级加速工具，七牛云与 Alluxio 官方团队深入合作，将其订制为更适应于深度学习场景。
  * 推荐使用如下硬件配置部署 Alluxio 服务的 master 节点:
    * CPU 36 cores
    * memory 256Gi 起
    * 240Gi SSD + 800Gi SSD
    * 万兆网卡或者千兆网卡皆可
  * 推荐使用如下硬件配置部署 Alluxio 服务的 worker 节点(或直接将 worker 部署在计算节点上):
    * cpu 36 cores
    * memory 256Gi 起
    * 240Gi SSD + 800Gi SSD * 6
    * 至少一张万兆网卡

### 计算节点

鉴于贵司已经采购多台 nvidia DGX-1 型号的主机，我们假定所有 GPU 主机都此型号，且都使用多块万兆网卡。如需使用多机多卡的分布式训练模式，可以加装无损网卡以提升跨主机间的 GPU 显卡通信性能。

除 GPU 主机外，为保证 AVA 平台的运行，需要其他一些 CPU 节点来支撑 kubernetes 等基础服务。推荐的硬件指标如下:

* kubernetes master 节点，3 台或者5台
  * CPU 36 cores
  * memory 128Gi 起
  * 240Gi SSD + 800Gi SSD
  * 万兆网卡或者千兆网卡皆可
* kubernetes DNS 节点，1台或者3台
  * cpu 36 cores
  * memory 128Gi 起
  * 240Gi SSD
  * 万兆网卡或者千兆网卡皆可
* 监控节点，3台(可以共用已有 Prometheus 服务，此服务需部署为高可用模式)
  * cpu 20 cores
  * memory 64Gi 起
  * 240Gi SSD
  * 万兆网卡或者千兆网卡皆可
* cpu 计算节点，从已有的运营经验来看，有一些任务可以只用 CPU 计算就能完成，建议部署 3 台
  * cpu 64 cores
  * memory 256Gi 起
  * 240Gi SSD + 800Gi SSD * 2
  * 万兆网卡

### 系统环境

所有节点采用相同的系统版本，其规格如下:

- ubuntu 16.04 发行版
- 4.18.5 kernel 版本

### 其他依赖

* mongo V3.2 数据库

* 镜像中心(可自建，也可使用七牛云提供的镜像服务)
* 一个二级域名

## 安装部署

### (一) 机房规划

经过讨论，此集群规模能控制在 256 个节点以内。上述规划的 CPU 服务器节点推荐都使用 2U 的机箱，IDC 规划时请预留一些机柜资源。请将所有在此系统中的节点都规划在同一机房，并使用同一网段管理，以有效降低网络链路对存储访问的影响。

### (二)  部署 Prometheus 监控服务

对 ceph / kubernetes 各节点的状态监控对系统的健康运转至关重要，如已有高可用模式的 Prometheus 服务，则可以利用现有的监控服务。

### (三) 搭建存储集群

在需要部署 ceph OSD 和 ceph MDS 等服务的节点上安装部署各 ceph 组件。

另外，可以使用其他对象存储服务如七牛对象存储 Kodo 私有化产品。

### (四) 搭建缓存系统

可以搭建针对 ceph 或者七牛对象存储系统 Alluxio 缓存系统。

### (五) 部署 kubernetes

* 在各 kubernetes 节点上部署 calico 网络组件。
* 选取合适的节点部署 Etcd 等基础依赖
* 在 GPU 计算节点上安装部署 GPU 显卡驱动及 nvidia-docker
* 分别在 kubernetes master/dns/node 等节点上部署所需的服务
* 创建 kubernetes 管理账号及各种权限分配
* 配置 kubernetes nodes 的角色(如: CPU类型/GPU)和资源 reserve 情况
* 在各 kubernetes node 节点部署 prometheus node-exporter，监控各节点的资源利用情况
* 在各 kubernetes node 节点部署 Alluxio 客户端驱动
* 部署 kubernetes ingress 组件

### (六) 部署 AVA 服务

* 部署 AVA 针对 kubernetes 定制的 operator/scheduler
* 将 AVA 各组件部署到 kubernetes 的 cpu 计算节点
* 创建 AVA 管理员账号
* 各用户注册 AVA 账号
* AVA 管理员账号对各用户分组，并配置 GPU/CPU/cephfs 等 Quota
* 部署各种 grafana 的监控看板，包括 ceph 运行状况、各 CPU/GPU 节点运行效率、各 AVA 用户的工作台/训练运行状态等，方便 AVA 系统管理员及用户了解系统状况
* 推送各种预先编译好的镜像等资源

## 上线运行

在系统部署完成后，将可以通过前端站点直接完成如下操作:

### (一) AVA 管理员

* 管理用户分组
* 管理分组的 CPU/GPU/cephfs 容量的 quota 等
* 查看各分组已经占用的 CPU/GPU 资源用量
* ava 用户可以使用的其他功能

### (二) AVA 用户

* 查看、创建、删除、分享自己的数据集
* 查看、创建、删除、运行、停止自己的工作台
* 查看、创建、删除、运行自己的训练，并查看训练监控及日志
* 查看自己所在的组占用的 CPU/GPU 资源用量
* 通过 JupterLab 直接登录工作台，执行各种命令
* 配置自己的 ssh public key 后，在本地命令行终端，ssh 登录到工作台，执行各种命令

## 系统验收

本次交付主要从存储系统的性能和 GPU 调度两个维度来验收系统的性能与效率，具体验收指标如下:

### 存储系统

ceph 系统主要使用 cephfs 和 ceph-rbd 两个部分，其验收指标分别如下:

#### cephfs

* iops 5000+ (请使用 10KB 的小文件测试)
* bandwith 1GB/s  (请使用1MB 以上的文件测试)
* 总文件数量 2 亿以上
* 单个目录下 100 万以上文件
* 多个客户端同时读写挂载，且性能不会明显下降

#### ceph-rbd

* iops 10000+
* bandwith 1GB/s
* 总文件数量 5 亿以上
* 单个目录下 1000 万以上文件
* 多个客户端同时只读挂载，且性能不会明显下降
* 单个客户端读写挂载

### 调度系统

* 创建的 CPU 类型工作台/训练只会调度到 CPU 节点上，GPU 类型工作台/训练只会调度到 GPU 节点上
* 工作台/训练停止后，GPU 资源可以在 5 分钟内释放出来，并再次供 ava 调度
* 在同一个 GPU  节点上的多个工作台/训练互不干扰，在工作台中使用 nvidia-smi 等工具只能查看到本身申请到的 GPU 资源，而不会使用其他工作台/训练里的 GPU 资源
* 当一个工作台中某个进程使用大量的内存，导致整个容器的内存超过自身申请的上限时，容器会根据 linux 自身的评分策略选出被杀掉的进程，而不会影响运行在同一个 GPU 节点上其他用户的工作台/训练
* 当一个训练任务尝试使用的内存超过自身申请的上限时，容器会被自动重启，而不会影响运行在同一个 GPU 节点上其他用户的工作台/训练
* 同一节点上各用户的工作台/训练容器之间文件系统互补干扰，不能互相访问
* 调度到 GPU 节点上时，默认选择能够满足申请资源且剩余资源最少的节点
* 每台机器上面已占用的 GPU 数量和闲置数量能在 grafana 看板上方便的查看，并在创建工作台/训练时顺利选中闲置的节点
* 可以在 grafana 上看到各用户正在运行的工作台/训练所占用的 GPU 利用率

### 其他特性

AVA 还在业务层面实现如下的功能

* 一个用户可以同时在多个分组中，同时只能在一个默认组下，其创建工作台/训练时占用当时所在的默认组的 quota
* 在用户启动的工作台/训练中，可以同时访问其所在的每个组的 cephfs 数据
* 用户可以在工作台或浏览器中查看其启动的训练任务的日志